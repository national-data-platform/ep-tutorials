{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# EarthScope GNSS Stations: Data Processing & Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/national-data-platform/ep-tutorials/blob/main/examples/earthscope-seismic-streaming.ipynb)\n",
    "\n",
    "## Why NDP-EP?\n",
    "\n",
    "This example demonstrates the **real advantage** of using NDP-EP for data management:\n",
    "\n",
    "- **User A** (first time): Downloads raw CSV from NDP catalog, cleans data, processes by region, stores in NDP-EP\n",
    "- **User B** (later): Finds processed data in NDP-EP, downloads directly - **much faster!**\n",
    "\n",
    "### Benefits\n",
    "\n",
    "| Without NDP-EP | With NDP-EP |\n",
    "|----------------|-------------|\n",
    "| Download 150 KB raw CSV | Download 5 KB processed JSON |\n",
    "| Clean malformed headers | Already cleaned |\n",
    "| Process 1100+ stations | Already aggregated by region |\n",
    "| ~10 seconds processing | Instant |\n",
    "\n",
    "## About EarthScope Data\n",
    "\n",
    "**EarthScope Consortium** provides high-rate (1Hz) GNSS position data from nearly 1,100 stations across the US. This data supports research on:\n",
    "- Earthquakes and seismic events\n",
    "- Volcanic activity\n",
    "- Ground deformation\n",
    "- Tectonic plate movement\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Access to an NDP-EP API instance\n",
    "- Valid authentication token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ndp-ep pandas matplotlib folium requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from ndp_ep import Client\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDP-EP Configuration\n",
    "API_URL = \"http://localhost:8002\"  # Change to your API endpoint\n",
    "TOKEN = \"testing_token\"            # Change to your token\n",
    "ORGANIZATION = \"earthscope-demo\"   # Organization for registering datasets\n",
    "\n",
    "# Dataset identifiers\n",
    "DATASET_NAME = \"earthscope-stations-processed\"\n",
    "BUCKET_NAME = \"seismic-data\"\n",
    "S3_KEY = \"earthscope/stations_by_region.json\"\n",
    "\n",
    "# Source data URL (from NDP Global Catalog)\n",
    "SOURCE_CSV_URL = \"https://nationaldataplatform.org/catalog/dataset/811f0bcc-99e5-455c-bcf6-7c63c2634f41/resource/a420cc30-2262-423a-8c63-3ad8d91f2a8f/download/earthscope_converted_data.csv\"\n",
    "\n",
    "# Initialize client\n",
    "client = Client(base_url=API_URL, token=TOKEN)\n",
    "\n",
    "# Verify connection\n",
    "status = client.get_system_status()\n",
    "\n",
    "print(f\"NDP-EP: {status['ep_name']} v{status['api_version']}\")\n",
    "print()\n",
    "print(\"Services status:\")\n",
    "print(f\"  - Local Catalog: {'✓ Connected' if status.get('backend_connected') else '✗ NOT CONNECTED'}\")\n",
    "print(f\"  - S3 Storage:    {'✓ Connected' if status.get('s3_connected') else '✗ NOT CONNECTED'}\")\n",
    "\n",
    "if not status.get('backend_connected'):\n",
    "    raise RuntimeError(\"Local catalog is not connected.\")\n",
    "if not status.get('s3_connected'):\n",
    "    raise RuntimeError(\"S3 storage is not connected.\")\n",
    "\n",
    "# Ensure organization exists\n",
    "existing_orgs = client.list_organizations(server=\"local\")\n",
    "if ORGANIZATION not in existing_orgs:\n",
    "    print(f\"\\nCreating organization '{ORGANIZATION}'...\")\n",
    "    client.register_organization({\n",
    "        \"name\": ORGANIZATION,\n",
    "        \"title\": \"EarthScope Demo Organization\",\n",
    "        \"description\": \"Demo organization for EarthScope seismic data\"\n",
    "    })\n",
    "    print(f\"  ✓ Created\")\n",
    "\n",
    "print(\"\\n✓ Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: Search for Existing Data\n",
    "\n",
    "Before downloading anything, let's check if someone has already processed EarthScope station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for existing processed data\n",
    "print(\"Searching for 'earthscope stations' in NDP-EP local catalog...\")\n",
    "print()\n",
    "\n",
    "results = client.search_datasets(terms=[\"earthscope\", \"stations\"], server=\"local\")\n",
    "\n",
    "# Look for our specific processed dataset\n",
    "found_dataset = None\n",
    "for dataset in results:\n",
    "    if dataset.get('name') == DATASET_NAME:\n",
    "        found_dataset = dataset\n",
    "        break\n",
    "\n",
    "if found_dataset:\n",
    "    print(f\"✓ Found existing dataset: {found_dataset['name']}\")\n",
    "    print(f\"  Title: {found_dataset.get('title', 'N/A')}\")\n",
    "    print(f\"\\n→ Skip to USER B section below!\")\n",
    "    DATA_EXISTS = True\n",
    "else:\n",
    "    print(\"✗ No processed EarthScope station data found in local catalog.\")\n",
    "    print(\"\\n→ Continue with USER A workflow to download and process.\")\n",
    "    DATA_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# USER A: First-Time Processing\n",
    "\n",
    "**Scenario**: No one has processed EarthScope station data yet. We need to:\n",
    "1. Download raw CSV from NDP catalog (150 KB, malformed headers)\n",
    "2. Clean and parse the data\n",
    "3. Process: aggregate stations by US region\n",
    "4. Upload processed JSON to NDP-EP S3\n",
    "5. Register in local catalog for others to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if data doesn't exist\n",
    "if DATA_EXISTS:\n",
    "    print(\"⏭ Data already exists. Skip to USER B section.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"USER A: Starting full processing workflow\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Start timing\n",
    "    user_a_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### A.1 Download Raw CSV from NDP Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    print(\"Downloading raw CSV from NDP catalog...\")\n",
    "    print(f\"Source: {SOURCE_CSV_URL[:60]}...\")\n",
    "    print()\n",
    "    \n",
    "    download_start = time.time()\n",
    "    \n",
    "    response = requests.get(SOURCE_CSV_URL, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    download_time = time.time() - download_start\n",
    "    download_kb = len(response.content) / 1024\n",
    "    \n",
    "    print(f\"✓ Downloaded {download_kb:.1f} KB in {download_time:.2f}s\")\n",
    "    \n",
    "    raw_csv = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### A.2 Clean and Parse Data\n",
    "\n",
    "The raw CSV has malformed headers with units embedded. We need to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    print(\"Cleaning and parsing raw data...\")\n",
    "    process_start = time.time()\n",
    "    \n",
    "    # Parse raw CSV - headers are malformed\n",
    "    # Format: Site,Latitude,(deg),Longitude,(deg),EllipElev,(m),X,(m),Y,(m),Z,(m),...\n",
    "    lines = raw_csv.strip().split('\\n')\n",
    "    \n",
    "    # Define correct column names\n",
    "    columns = [\n",
    "        'site', 'latitude', 'longitude', 'elevation_m',\n",
    "        'x_m', 'y_m', 'z_m', 'epoch_yr', 'network',\n",
    "        'status', 'last_update', 'antenna_height_m', 'antenna_type', 'dome'\n",
    "    ]\n",
    "    \n",
    "    # Parse data rows\n",
    "    data = []\n",
    "    for line in lines[1:]:  # Skip header\n",
    "        parts = line.split(',')\n",
    "        if len(parts) >= 14:\n",
    "            try:\n",
    "                row = {\n",
    "                    'site': parts[0],\n",
    "                    'latitude': float(parts[1]),\n",
    "                    'longitude': float(parts[2]),\n",
    "                    'elevation_m': float(parts[3]),\n",
    "                    'x_m': float(parts[4]),\n",
    "                    'y_m': float(parts[5]),\n",
    "                    'z_m': float(parts[6]),\n",
    "                    'epoch_yr': float(parts[7]),\n",
    "                    'network': parts[8],\n",
    "                    'status': parts[9],\n",
    "                    'last_update': parts[10],\n",
    "                    'antenna_height_m': float(parts[11]) if parts[11] else 0,\n",
    "                    'antenna_type': parts[12],\n",
    "                    'dome': parts[13]\n",
    "                }\n",
    "                data.append(row)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"\\n✓ Parsed {len(df)} stations\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print(f\"  Networks: {df['network'].nunique()} unique\")\n",
    "    print(f\"  Active stations: {len(df[df['status'] == 'ACTIVE'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### A.3 Process: Aggregate by US Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "if not DATA_EXISTS:\n    print(\"Processing: Aggregating stations by US region...\")\n    \n    # Define US regions by longitude/latitude\n    def get_region(lat, lon):\n        if lon > -100:  # East of -100\n            if lat > 40:\n                return 'Northeast'\n            else:\n                return 'Southeast'\n        else:  # West of -100\n            if lat > 45:\n                return 'Pacific Northwest'\n            elif lat > 35:\n                return 'California'\n            else:\n                return 'Southwest'\n    \n    # Only include US stations (continental)\n    df_us = df[(df['latitude'] > 24) & (df['latitude'] < 50) & \n               (df['longitude'] > -125) & (df['longitude'] < -66)].copy()\n    \n    df_us['region'] = df_us.apply(lambda r: get_region(r['latitude'], r['longitude']), axis=1)\n    \n    # Aggregate statistics by region\n    region_stats = {}\n    for region in df_us['region'].unique():\n        region_df = df_us[df_us['region'] == region]\n        \n        # Only include sample of stations (10 per region) to keep JSON small\n        sample_stations = region_df[['site', 'latitude', 'longitude', 'elevation_m', 'network', 'status']].head(10).to_dict('records')\n        \n        region_stats[region] = {\n            'station_count': len(region_df),\n            'active_count': len(region_df[region_df['status'] == 'ACTIVE']),\n            'networks': region_df['network'].unique().tolist(),\n            'avg_elevation_m': round(region_df['elevation_m'].mean(), 2),\n            'min_elevation_m': round(region_df['elevation_m'].min(), 2),\n            'max_elevation_m': round(region_df['elevation_m'].max(), 2),\n            'center_lat': round(region_df['latitude'].mean(), 4),\n            'center_lon': round(region_df['longitude'].mean(), 4),\n            'sample_stations': sample_stations  # Only 10 samples, not all\n        }\n    \n    # Create processed dataset\n    processed_data = {\n        'metadata': {\n            'source': 'EarthScope Consortium',\n            'processed_date': time.strftime('%Y-%m-%d'),\n            'total_stations': len(df_us),\n            'regions': list(region_stats.keys())\n        },\n        'regions': region_stats\n    }\n    \n    process_time = time.time() - process_start\n    \n    print(f\"\\n✓ Processing complete in {process_time:.2f}s\")\n    print(f\"\\nRegion summary:\")\n    for region, stats in region_stats.items():\n        print(f\"  {region}: {stats['station_count']} stations ({stats['active_count']} active)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### A.4 Upload to NDP-EP S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    print(\"Uploading processed data to NDP-EP S3...\")\n",
    "    \n",
    "    # Create bucket\n",
    "    try:\n",
    "        client.create_bucket(BUCKET_NAME)\n",
    "        print(f\"  ✓ Created bucket: {BUCKET_NAME}\")\n",
    "    except:\n",
    "        print(f\"  ✓ Bucket exists: {BUCKET_NAME}\")\n",
    "    \n",
    "    # Convert to JSON and upload\n",
    "    json_data = json.dumps(processed_data, indent=2)\n",
    "    processed_kb = len(json_data.encode()) / 1024\n",
    "    \n",
    "    upload_start = time.time()\n",
    "    client.upload_object(BUCKET_NAME, S3_KEY, json_data.encode(), \"application/json\")\n",
    "    upload_time = time.time() - upload_start\n",
    "    \n",
    "    print(f\"  ✓ Uploaded {processed_kb:.1f} KB in {upload_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### A.5 Register in Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    print(\"Registering dataset in NDP-EP catalog...\")\n",
    "    \n",
    "    result = client.register_s3_link({\n",
    "        \"resource_name\": DATASET_NAME,\n",
    "        \"resource_title\": \"EarthScope GNSS Stations - Processed by Region\",\n",
    "        \"owner_org\": ORGANIZATION,\n",
    "        \"s3_bucket\": BUCKET_NAME,\n",
    "        \"s3_key\": S3_KEY,\n",
    "        \"resource_s3\": f\"{BUCKET_NAME}/{S3_KEY}\",\n",
    "        \"notes\": \"Pre-processed EarthScope GNSS station data aggregated by US region. Includes station counts, elevation statistics, and network information.\"\n",
    "    })\n",
    "    \n",
    "    print(f\"  ✓ Registered: {result}\")\n",
    "    \n",
    "    # Total time for User A\n",
    "    user_a_total = time.time() - user_a_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USER A SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Download raw CSV:    {download_time:.2f}s ({download_kb:.1f} KB)\")\n",
    "    print(f\"  Clean & Process:     {process_time:.2f}s\")\n",
    "    print(f\"  Upload to NDP-EP:    {upload_time:.2f}s ({processed_kb:.1f} KB)\")\n",
    "    print(f\"  ─────────────────────────────\")\n",
    "    print(f\"  TOTAL TIME:          {user_a_total:.2f}s\")\n",
    "    print(f\"  Data downloaded:     {download_kb:.1f} KB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Store for comparison\n",
    "    USER_A_TIME = user_a_total\n",
    "    USER_A_KB = download_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# USER B: Fast Access (Data Already in NDP-EP)\n",
    "\n",
    "**Scenario**: Another user (or the same user later) needs EarthScope station data by region.\n",
    "\n",
    "Instead of repeating all the work, they:\n",
    "1. Search NDP-EP → Find existing processed dataset\n",
    "2. Download processed JSON directly (only ~5 KB!)\n",
    "3. Analyze immediately - no cleaning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"USER B: Fast workflow (data already in NDP-EP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "user_b_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### B.1 Search Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searching NDP-EP catalog for 'earthscope stations'...\")\n",
    "\n",
    "search_start = time.time()\n",
    "results = client.search_datasets(terms=[\"earthscope\", \"stations\"], server=\"local\")\n",
    "search_time = time.time() - search_start\n",
    "\n",
    "# Find our dataset\n",
    "found = None\n",
    "for dataset in results:\n",
    "    if dataset.get('name') == DATASET_NAME:\n",
    "        found = dataset\n",
    "        break\n",
    "\n",
    "if found:\n",
    "    print(f\"\\n✓ Found in {search_time:.3f}s: {found['name']}\")\n",
    "    print(f\"  Title: {found.get('title', 'N/A')}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Dataset not found. Run USER A section first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### B.2 Download from NDP-EP (Fast!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading processed data from NDP-EP S3...\")\n",
    "\n",
    "download_start = time.time()\n",
    "\n",
    "# Download directly from NDP-EP S3\n",
    "data = client.download_object(BUCKET_NAME, S3_KEY)\n",
    "\n",
    "download_time_b = time.time() - download_start\n",
    "download_kb_b = len(data) / 1024\n",
    "\n",
    "# Parse JSON\n",
    "processed_data = json.loads(data.decode())\n",
    "\n",
    "print(f\"\\n✓ Downloaded {download_kb_b:.2f} KB in {download_time_b:.3f}s\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Source: {processed_data['metadata']['source']}\")\n",
    "print(f\"  Processed: {processed_data['metadata']['processed_date']}\")\n",
    "print(f\"  Total stations: {processed_data['metadata']['total_stations']}\")\n",
    "print(f\"  Regions: {processed_data['metadata']['regions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### B.3 Analyze Immediately (No Processing Needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing regional data...\\n\")\n",
    "\n",
    "# Data is already aggregated - just display\n",
    "print(f\"{'Region':<20} {'Stations':<10} {'Active':<10} {'Avg Elev (m)':<15} {'Networks'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for region, stats in processed_data['regions'].items():\n",
    "    networks = ', '.join(stats['networks'][:3])\n",
    "    if len(stats['networks']) > 3:\n",
    "        networks += f\" +{len(stats['networks'])-3} more\"\n",
    "    print(f\"{region:<20} {stats['station_count']:<10} {stats['active_count']:<10} {stats['avg_elevation_m']:<15} {networks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### B.4 Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Creating visualization...\")\nviz_start = time.time()\n\n# Create map\nm = folium.Map(location=[39.0, -98.0], zoom_start=4, tiles='CartoDB positron')\n\n# Color by region\ncolors = {\n    'California': 'red',\n    'Pacific Northwest': 'blue',\n    'Southwest': 'orange',\n    'Northeast': 'green',\n    'Southeast': 'purple'\n}\n\n# Add stations from processed data\nfor region, stats in processed_data['regions'].items():\n    color = colors.get(region, 'gray')\n    \n    # Add region center marker\n    folium.Marker(\n        location=[stats['center_lat'], stats['center_lon']],\n        popup=f\"<b>{region}</b><br>{stats['station_count']} stations\",\n        icon=folium.Icon(color=color, icon='info-sign')\n    ).add_to(m)\n    \n    # Add sample station markers\n    for station in stats.get('sample_stations', []):\n        folium.CircleMarker(\n            location=[station['latitude'], station['longitude']],\n            radius=3,\n            color=color,\n            fill=True,\n            fillOpacity=0.7,\n            popup=f\"{station['site']} ({station['network']})\"\n        ).add_to(m)\n\n# Add legend\nlegend_html = '''\n<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white; padding: 10px; border-radius: 5px; border: 2px solid gray;\">\n<b>Regions</b><br>\n''' + ''.join([f'<i style=\"background:{c};width:10px;height:10px;display:inline-block;margin-right:5px;\"></i>{r}<br>' for r, c in colors.items()]) + '</div>'\n\nm.get_root().html.add_child(folium.Element(legend_html))\n\nviz_time = time.time() - viz_start\nprint(f\"\\n✓ Visualization ready in {viz_time:.2f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User B total time\n",
    "user_b_total = time.time() - user_b_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USER B SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Search catalog:      {search_time:.3f}s\")\n",
    "print(f\"  Download from S3:    {download_time_b:.3f}s ({download_kb_b:.2f} KB)\")\n",
    "print(f\"  Visualization:       {viz_time:.2f}s\")\n",
    "print(f\"  ─────────────────────────────\")\n",
    "print(f\"  TOTAL TIME:          {user_b_total:.2f}s\")\n",
    "print(f\"  Data downloaded:     {download_kb_b:.2f} KB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "USER_B_TIME = user_b_total\n",
    "USER_B_KB = download_kb_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comparison: User A vs User B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get User A values if available, otherwise use typical values\n",
    "try:\n",
    "    ua_time = USER_A_TIME\n",
    "    ua_kb = USER_A_KB\n",
    "except:\n",
    "    ua_time = 8.0   # Typical time\n",
    "    ua_kb = 149.5   # Typical download size\n",
    "\n",
    "ub_time = USER_B_TIME\n",
    "ub_kb = USER_B_KB\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: NDP-EP BENEFITS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"                    USER A          USER B\")\n",
    "print(f\"                    (first time)    (with NDP-EP)\")\n",
    "print(f\"  ──────────────────────────────────────────────\")\n",
    "print(f\"  Time:             {ua_time:>6.2f}s         {ub_time:>6.2f}s\")\n",
    "print(f\"  Data downloaded:  {ua_kb:>6.1f} KB       {ub_kb:>6.2f} KB\")\n",
    "print(f\"  Processing:       Required        None\")\n",
    "print(f\"  Data cleaning:    Required        None\")\n",
    "print(f\"  ──────────────────────────────────────────────\")\n",
    "print()\n",
    "if ua_time > ub_time:\n",
    "    print(f\"  Time saved:      {ua_time - ub_time:.2f}s ({(1 - ub_time/ua_time)*100:.0f}% faster)\")\n",
    "if ua_kb > ub_kb:\n",
    "    print(f\"  Data saved:      {ua_kb - ub_kb:.1f} KB ({(1 - ub_kb/ua_kb)*100:.0f}% less)\")\n",
    "print()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Why Use NDP-EP?\n",
    "\n",
    "**1. Avoid Redundant Work**\n",
    "- Process data once, use many times\n",
    "- No need to re-download and re-clean raw data\n",
    "\n",
    "**2. Faster Access**\n",
    "- Local S3 is faster than external sources\n",
    "- Pre-processed data is smaller and cleaner\n",
    "\n",
    "**3. Data Discovery**\n",
    "- Search catalog before downloading\n",
    "- Find datasets others have already prepared\n",
    "\n",
    "**4. Collaboration**\n",
    "- Share processed data with your team\n",
    "- Standard metadata for discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all resources\n",
    "\n",
    "# client.delete_resource_by_name(DATASET_NAME)\n",
    "# client.delete_object(BUCKET_NAME, S3_KEY)\n",
    "# client.delete_bucket(BUCKET_NAME)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}