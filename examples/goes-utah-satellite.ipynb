{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GOES-18 Satellite Imagery: Utah Region\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/national-data-platform/ep-tutorials/blob/main/examples/goes-utah-satellite.ipynb)\n",
    "\n",
    "## Why NDP-EP?\n",
    "\n",
    "This example demonstrates the **real advantage** of using NDP-EP for data management:\n",
    "\n",
    "- **User A** (first time): Downloads from AWS, processes, registers in NDP-EP\n",
    "- **User B** (later): Finds processed data in NDP-EP, downloads directly - **much faster!**\n",
    "\n",
    "### Benefits\n",
    "\n",
    "| Without NDP-EP | With NDP-EP |\n",
    "|----------------|-------------|\n",
    "| Download 57 MB from AWS every time | Download 1 MB from local S3 |\n",
    "| Process data every time | Already processed |\n",
    "| ~60 seconds | ~2 seconds |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Access to an NDP-EP API instance\n",
    "- Valid authentication token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ndp-ep xarray netCDF4 pyproj folium numpy requests pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import requests\n",
    "import folium\n",
    "from pyproj import CRS, Transformer\n",
    "from ndp_ep import Client\n",
    "import tempfile\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDP-EP Configuration\n",
    "API_URL = \"http://localhost:8002\"  # Change to your API endpoint\n",
    "TOKEN = \"testing_token\"            # Change to your token\n",
    "ORGANIZATION = \"noaa-demo\"         # Organization for registering datasets\n",
    "\n",
    "# Dataset identifiers\n",
    "DATASET_NAME = \"goes18-utah-processed\"\n",
    "BUCKET_NAME = \"satellite-data\"\n",
    "S3_KEY = \"goes18/utah/goes18_utah_20241125_1800.nc\"\n",
    "\n",
    "# Initialize client\n",
    "client = Client(base_url=API_URL, token=TOKEN)\n",
    "\n",
    "# Verify connection\n",
    "status = client.get_system_status()\n",
    "\n",
    "print(f\"NDP-EP: {status['ep_name']} v{status['api_version']}\")\n",
    "print(f\"\")\n",
    "print(\"Services status:\")\n",
    "print(f\"  - Local Catalog: {'âœ“ Connected' if status.get('backend_connected') else 'âœ— NOT CONNECTED'}\")\n",
    "print(f\"  - S3 Storage:    {'âœ“ Connected' if status.get('s3_connected') else 'âœ— NOT CONNECTED'}\")\n",
    "\n",
    "if not status.get('backend_connected'):\n",
    "    raise RuntimeError(\"âŒ Local catalog is not connected.\")\n",
    "if not status.get('s3_connected'):\n",
    "    raise RuntimeError(\"âŒ S3 storage is not connected.\")\n",
    "\n",
    "# Ensure organization exists\n",
    "existing_orgs = client.list_organizations(server=\"local\")\n",
    "if ORGANIZATION not in existing_orgs:\n",
    "    print(f\"\\nCreating organization '{ORGANIZATION}'...\")\n",
    "    client.register_organization({\n",
    "        \"name\": ORGANIZATION,\n",
    "        \"title\": \"NOAA Demo Organization\",\n",
    "        \"description\": \"Demo organization for NOAA satellite data\"\n",
    "    })\n",
    "    print(f\"  âœ“ Created\")\n",
    "\n",
    "print(\"\\nâœ“ Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory for this session\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Working directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ” Step 1: Search for Existing Data\n",
    "\n",
    "Before downloading anything, let's check if someone has already processed GOES-18 data for Utah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for existing Utah GOES data\n",
    "print(\"Searching for 'GOES utah' in NDP-EP catalog...\")\n",
    "print()\n",
    "\n",
    "results = client.search_datasets(terms=[\"goes18\", \"utah\"], server=\"local\")\n",
    "\n",
    "# Look for our specific processed dataset\n",
    "found_dataset = None\n",
    "for dataset in results:\n",
    "    if dataset.get('name') == DATASET_NAME:\n",
    "        found_dataset = dataset\n",
    "        break\n",
    "\n",
    "if found_dataset:\n",
    "    print(f\"âœ“ Found existing dataset: {found_dataset['name']}\")\n",
    "    print(f\"  Title: {found_dataset.get('title', 'N/A')}\")\n",
    "    print(f\"\\nâ†’ Skip to USER B section below!\")\n",
    "    DATA_EXISTS = True\n",
    "else:\n",
    "    print(\"âœ— No processed GOES-18 Utah data found in catalog.\")\n",
    "    print(\"\\nâ†’ Continue with USER A workflow to download and process.\")\n",
    "    DATA_EXISTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ‘¤ USER A: First-Time Processing\n",
    "\n",
    "**Scenario**: No one has processed Utah GOES data yet. We need to:\n",
    "1. Download from AWS (slow, 57 MB)\n",
    "2. Process and crop to Utah\n",
    "3. Upload to NDP-EP S3\n",
    "4. Register in catalog for others to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if data doesn't exist\n",
    "if DATA_EXISTS:\n",
    "    print(\"â­ Data already exists. Skip to USER B section.\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"USER A: Starting full processing workflow\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Start timing\n",
    "    user_a_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### A.1 Download from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    # GOES-18 file on AWS\n",
    "    GOES_FILE_URL = \"https://noaa-goes18.s3.amazonaws.com/ABI-L2-MCMIPC/2024/330/18/OR_ABI-L2-MCMIPC-M6_G18_s20243301801170_e20243301803543_c20243301804085.nc\"\n",
    "    \n",
    "    print(\"Downloading from AWS S3 (NOAA public bucket)...\")\n",
    "    print(f\"Source: {GOES_FILE_URL[:60]}...\")\n",
    "    print()\n",
    "    \n",
    "    download_start = time.time()\n",
    "    \n",
    "    goes_file = os.path.join(temp_dir, \"goes18_conus.nc\")\n",
    "    response = requests.get(GOES_FILE_URL, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    downloaded_bytes = 0\n",
    "    with open(goes_file, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "            downloaded_bytes += len(chunk)\n",
    "            if total_size:\n",
    "                pct = (downloaded_bytes / total_size) * 100\n",
    "                print(f\"\\rDownloading: {downloaded_bytes/1e6:.1f}/{total_size/1e6:.1f} MB ({pct:.0f}%)\", end=\"\")\n",
    "    \n",
    "    download_time = time.time() - download_start\n",
    "    download_mb = downloaded_bytes / 1e6\n",
    "    \n",
    "    print(f\"\\n\\nâœ“ Downloaded {download_mb:.1f} MB in {download_time:.1f} seconds\")\n",
    "    print(f\"  Speed: {download_mb/download_time:.1f} MB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### A.2 Process: Crop to Utah Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    process_start = time.time()\n",
    "    \n",
    "    # Utah geographic bounds\n",
    "    UTAH_BOUNDS = {\n",
    "        'lat_min': 37.0, 'lat_max': 42.0,\n",
    "        'lon_min': -114.0, 'lon_max': -109.0\n",
    "    }\n",
    "    \n",
    "    print(\"Processing GOES-18 data...\")\n",
    "    print(f\"  Cropping to Utah: {UTAH_BOUNDS}\")\n",
    "    \n",
    "    # Open and read projection info\n",
    "    goes_ds = xr.open_dataset(goes_file)\n",
    "    proj = goes_ds['goes_imager_projection']\n",
    "    h = proj.attrs['perspective_point_height']\n",
    "    lon_0 = proj.attrs['longitude_of_projection_origin']\n",
    "    sweep = proj.attrs.get('sweep_angle_axis', 'x')\n",
    "    \n",
    "    # Create coordinate transformation\n",
    "    goes_crs = CRS.from_cf({\n",
    "        'grid_mapping_name': 'geostationary',\n",
    "        'longitude_of_projection_origin': lon_0,\n",
    "        'perspective_point_height': h,\n",
    "        'sweep_angle_axis': sweep,\n",
    "        'semi_major_axis': proj.attrs['semi_major_axis'],\n",
    "        'semi_minor_axis': proj.attrs['semi_minor_axis'],\n",
    "    })\n",
    "    transformer = Transformer.from_crs(goes_crs, \"EPSG:4326\", always_xy=True)\n",
    "    \n",
    "    # Transform coordinates\n",
    "    x = goes_ds['x'].values * h\n",
    "    y = goes_ds['y'].values * h\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    lon, lat = transformer.transform(xx, yy)\n",
    "    \n",
    "    # Find Utah region\n",
    "    utah_mask = (\n",
    "        (lat >= UTAH_BOUNDS['lat_min']) & (lat <= UTAH_BOUNDS['lat_max']) &\n",
    "        (lon >= UTAH_BOUNDS['lon_min']) & (lon <= UTAH_BOUNDS['lon_max'])\n",
    "    )\n",
    "    utah_indices = np.where(utah_mask)\n",
    "    y_min, y_max = utah_indices[0].min(), utah_indices[0].max()\n",
    "    x_min, x_max = utah_indices[1].min(), utah_indices[1].max()\n",
    "    \n",
    "    # Extract Utah subset\n",
    "    cmi_utah = goes_ds['CMI_C02'].values[y_min:y_max, x_min:x_max]\n",
    "    lat_utah = lat[y_min:y_max, x_min:x_max]\n",
    "    lon_utah = lon[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # Save as NetCDF\n",
    "    utah_file = os.path.join(temp_dir, \"goes18_utah.nc\")\n",
    "    utah_ds = xr.Dataset({\n",
    "        'reflectance': (['y', 'x'], cmi_utah),\n",
    "        'latitude': (['y', 'x'], lat_utah),\n",
    "        'longitude': (['y', 'x'], lon_utah),\n",
    "    })\n",
    "    utah_ds.attrs = {\n",
    "        'title': 'GOES-18 Utah Region Subset',\n",
    "        'source': 'NOAA GOES-18 ABI L2 MCMIPC',\n",
    "        'band': 'C02 (Red, 0.64 Âµm)',\n",
    "        'bounds': str(UTAH_BOUNDS),\n",
    "    }\n",
    "    utah_ds.to_netcdf(utah_file)\n",
    "    \n",
    "    goes_ds.close()\n",
    "    \n",
    "    process_time = time.time() - process_start\n",
    "    processed_mb = os.path.getsize(utah_file) / 1e6\n",
    "    \n",
    "    print(f\"\\nâœ“ Processing complete in {process_time:.1f} seconds\")\n",
    "    print(f\"  Original: {download_mb:.1f} MB â†’ Processed: {processed_mb:.2f} MB\")\n",
    "    print(f\"  Reduction: {(1 - processed_mb/download_mb)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### A.3 Upload to NDP-EP S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    print(\"Uploading processed data to NDP-EP S3...\")\n",
    "    \n",
    "    # Create bucket\n",
    "    try:\n",
    "        client.create_bucket(BUCKET_NAME)\n",
    "        print(f\"  âœ“ Created bucket: {BUCKET_NAME}\")\n",
    "    except:\n",
    "        print(f\"  âœ“ Bucket exists: {BUCKET_NAME}\")\n",
    "    \n",
    "    # Upload\n",
    "    upload_start = time.time()\n",
    "    with open(utah_file, 'rb') as f:\n",
    "        client.upload_object(BUCKET_NAME, S3_KEY, f.read(), \"application/x-netcdf\")\n",
    "    upload_time = time.time() - upload_start\n",
    "    \n",
    "    print(f\"  âœ“ Uploaded {processed_mb:.2f} MB in {upload_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### A.4 Register in Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_EXISTS:\n",
    "    print(\"Registering dataset in NDP-EP catalog...\")\n",
    "    \n",
    "    result = client.register_s3_link({\n",
    "        \"resource_name\": DATASET_NAME,\n",
    "        \"resource_title\": \"GOES-18 Utah Region - Visible Band (Processed)\",\n",
    "        \"owner_org\": ORGANIZATION,\n",
    "        \"s3_bucket\": BUCKET_NAME,\n",
    "        \"s3_key\": S3_KEY,\n",
    "        \"resource_s3\": f\"{BUCKET_NAME}/{S3_KEY}\",\n",
    "        \"notes\": \"Pre-processed GOES-18 visible band imagery cropped to Utah region (37-42Â°N, 109-114Â°W). Ready for direct visualization.\"\n",
    "    })\n",
    "    \n",
    "    print(f\"  âœ“ Registered: {result}\")\n",
    "    \n",
    "    # Total time for User A\n",
    "    user_a_total = time.time() - user_a_start\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USER A SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Download from AWS:  {download_time:.1f}s ({download_mb:.1f} MB)\")\n",
    "    print(f\"  Processing:         {process_time:.1f}s\")\n",
    "    print(f\"  Upload to NDP-EP:   {upload_time:.1f}s\")\n",
    "    print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "    print(f\"  TOTAL TIME:         {user_a_total:.1f}s\")\n",
    "    print(f\"  Data downloaded:    {download_mb:.1f} MB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Store for comparison\n",
    "    USER_A_TIME = user_a_total\n",
    "    USER_A_MB = download_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ‘¤ USER B: Fast Access (Data Already in NDP-EP)\n",
    "\n",
    "**Scenario**: Another user (or the same user later) needs Utah GOES data.\n",
    "\n",
    "Instead of repeating all the work, they:\n",
    "1. Search NDP-EP â†’ Find existing dataset\n",
    "2. Download processed data directly (only 1 MB!)\n",
    "3. Visualize immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"USER B: Fast workflow (data already in NDP-EP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "user_b_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### B.1 Search Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searching NDP-EP catalog for 'GOES utah'...\")\n",
    "\n",
    "search_start = time.time()\n",
    "results = client.search_datasets(terms=[\"goes18\", \"utah\"], server=\"local\")\n",
    "search_time = time.time() - search_start\n",
    "\n",
    "# Find our dataset\n",
    "found = None\n",
    "for dataset in results:\n",
    "    if dataset.get('name') == DATASET_NAME:\n",
    "        found = dataset\n",
    "        break\n",
    "\n",
    "if found:\n",
    "    print(f\"\\nâœ“ Found in {search_time:.2f}s: {found['name']}\")\n",
    "    print(f\"  Title: {found.get('title', 'N/A')}\")\n",
    "else:\n",
    "    raise RuntimeError(\"Dataset not found. Run USER A section first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### B.2 Download from NDP-EP (Fast!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading processed data from NDP-EP S3...\")\n",
    "\n",
    "download_start = time.time()\n",
    "\n",
    "# Download directly from NDP-EP S3\n",
    "data = client.download_object(BUCKET_NAME, S3_KEY)\n",
    "\n",
    "# Save locally\n",
    "utah_file_b = os.path.join(temp_dir, \"goes18_utah_from_ndpep.nc\")\n",
    "with open(utah_file_b, 'wb') as f:\n",
    "    f.write(data)\n",
    "\n",
    "download_time_b = time.time() - download_start\n",
    "download_mb_b = len(data) / 1e6\n",
    "\n",
    "print(f\"\\nâœ“ Downloaded {download_mb_b:.2f} MB in {download_time_b:.2f}s\")\n",
    "print(f\"  Speed: {download_mb_b/download_time_b:.1f} MB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### B.3 Load and Visualize (No Processing Needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data for visualization...\")\n",
    "\n",
    "viz_start = time.time()\n",
    "\n",
    "# Load the pre-processed data\n",
    "ds_utah = xr.open_dataset(utah_file_b)\n",
    "\n",
    "cmi_utah = ds_utah['reflectance'].values\n",
    "lat_utah = ds_utah['latitude'].values\n",
    "lon_utah = ds_utah['longitude'].values\n",
    "\n",
    "print(f\"  Shape: {cmi_utah.shape}\")\n",
    "print(f\"  Reflectance range: {np.nanmin(cmi_utah):.3f} to {np.nanmax(cmi_utah):.3f}\")\n",
    "\n",
    "ds_utah.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "# Normalize and apply gamma correction\n",
    "cmi_normalized = np.nan_to_num(np.clip(cmi_utah, 0, 1), nan=0)\n",
    "cmi_uint8 = (np.power(cmi_normalized, 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "# Save PNG\n",
    "img = Image.fromarray(cmi_uint8, mode='L')\n",
    "png_file = os.path.join(temp_dir, \"utah_satellite_b.png\")\n",
    "img.save(png_file)\n",
    "\n",
    "# Create Folium map\n",
    "bounds = [\n",
    "    [float(np.nanmin(lat_utah)), float(np.nanmin(lon_utah))],\n",
    "    [float(np.nanmax(lat_utah)), float(np.nanmax(lon_utah))]\n",
    "]\n",
    "\n",
    "m = folium.Map(location=[39.5, -111.5], zoom_start=7, tiles='OpenStreetMap')\n",
    "\n",
    "with open(png_file, 'rb') as f:\n",
    "    img_data = base64.b64encode(f.read()).decode()\n",
    "\n",
    "folium.raster_layers.ImageOverlay(\n",
    "    image=f\"data:image/png;base64,{img_data}\",\n",
    "    bounds=bounds,\n",
    "    opacity=0.7,\n",
    "    name=\"GOES-18 Visible Band\"\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "viz_time = time.time() - viz_start\n",
    "\n",
    "print(f\"\\nâœ“ Visualization ready in {viz_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User B total time\n",
    "user_b_total = time.time() - user_b_start\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USER B SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Search catalog:     {search_time:.2f}s\")\n",
    "print(f\"  Download from S3:   {download_time_b:.2f}s ({download_mb_b:.2f} MB)\")\n",
    "print(f\"  Visualization:      {viz_time:.2f}s\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  TOTAL TIME:         {user_b_total:.1f}s\")\n",
    "print(f\"  Data downloaded:    {download_mb_b:.2f} MB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "USER_B_TIME = user_b_total\n",
    "USER_B_MB = download_mb_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“Š Comparison: User A vs User B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get User A values if available, otherwise use typical values\n",
    "try:\n",
    "    ua_time = USER_A_TIME\n",
    "    ua_mb = USER_A_MB\n",
    "except:\n",
    "    ua_time = 45.0  # Typical time\n",
    "    ua_mb = 57.3    # Typical download size\n",
    "\n",
    "ub_time = USER_B_TIME\n",
    "ub_mb = USER_B_MB\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š COMPARISON: NDP-EP BENEFITS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"                    USER A          USER B\")\n",
    "print(f\"                    (first time)    (with NDP-EP)\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  Time:             {ua_time:>6.1f}s         {ub_time:>6.1f}s\")\n",
    "print(f\"  Data downloaded:  {ua_mb:>6.1f} MB       {ub_mb:>6.2f} MB\")\n",
    "print(f\"  Processing:       Required        None\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print()\n",
    "print(f\"  â±  Time saved:     {ua_time - ub_time:.1f}s ({(1 - ub_time/ua_time)*100:.0f}% faster)\")\n",
    "print(f\"  ğŸ“¦ Data saved:     {ua_mb - ub_mb:.1f} MB ({(1 - ub_mb/ua_mb)*100:.0f}% less)\")\n",
    "print()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Why Use NDP-EP?\n",
    "\n",
    "**1. Avoid Redundant Work**\n",
    "- Process data once, use many times\n",
    "- No need to re-download from slow external sources\n",
    "\n",
    "**2. Faster Access**\n",
    "- Local S3 is faster than AWS public buckets\n",
    "- Pre-processed data is smaller\n",
    "\n",
    "**3. Data Discovery**\n",
    "- Search catalog before downloading\n",
    "- Find datasets others have already prepared\n",
    "\n",
    "**4. Collaboration**\n",
    "- Share processed data with your team\n",
    "- Standard metadata for discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all resources\n",
    "\n",
    "# client.delete_resource_by_name(DATASET_NAME)\n",
    "# client.delete_object(BUCKET_NAME, S3_KEY)\n",
    "# client.delete_bucket(BUCKET_NAME)\n",
    "\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
